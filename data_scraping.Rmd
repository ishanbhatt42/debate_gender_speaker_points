---
title: "data_scraping"
author: "Ishan Bhatt"
date: "4/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(rvest)
library(janitor)
library(reprex)
library(gender)
library(gt)
library(readr)
library(broom)
library(readr)

```

### Writing the Functions

```{r writing the functions}

# The functions I write here are different
# because different tournaments upload their
# data differently. Each one attempts to result in 
# a data set with the same variables in the same
# format so they can be merged.

scrape <- function(tournament, year, link) {
  
# Grab the table and store it as a data 
# frame. Clean the names and add two columns 
# using the tournament and year arguments.
  
x <- read_html(link) %>%
  html_nodes("table") %>%
  html_table() %>%
  as.data.frame() %>%
  clean_names() %>%
  mutate(tourn = tournament) %>%
  mutate(season = year)

# Create the gender table for the 
# first names. Select out the data that 
# we don't need - we only need the first name
# and the gender.

x_gendered <- gender(names = x$first, years = c(1995, 2005), method = c(
  "ssa", "ipums", "napp",
  "kantrowitz", "genderize", "demo")) %>%
  select(-proportion_male, -proportion_female, -year_min, -year_max)

# Join it with the earlier tibble and select
# a uniform number of columns so we can stack
# tournaments on top of each other.

x %>%
  left_join(x_gendered, by = c("first" = "name")) %>%
  distinct() %>%
  select(first, last, entry, state, pts_1hl, tourn, season, gender)

}

# Same deal with this function, but some tournaments upload their
# data with the names not separated into first and last.
# The only difference here is I've added a "separate" which
# takes the first name.

scrape_names <- function(tournament, year, link){

x <- read_html(link) %>%
  html_nodes("table") %>%
  html_table() %>%
  as.data.frame() %>%
  clean_names() %>%
  separate(name, into = c("first", "last"), sep = " ") %>%
  mutate(tourn = tournament) %>%
  mutate(season = year)

x_gendered <- gender(names = x$first, years = c(1995, 2005), method = c(
  "ssa", "ipums", "napp",
  "kantrowitz", "genderize", "demo")) %>%
  select(-proportion_male, -proportion_female, -year_min, -year_max)

x %>%
  left_join(x_gendered, by = c("first" = "name")) %>%
  distinct() %>%
  select(first, last, entry = code, state, pts_1hl = pts_pm_1hl, tourn, season, gender)

}

# Same deal again, but for some stupid tournaments
# that try to be special by labeling their data
# different things.

scrape_p_pts <- function(tournament, year, link){

x <- read_html(link) %>%
  html_nodes("table") %>%
  html_table() %>%
  as.data.frame() %>%
  clean_names() %>%
  separate(name, into = c("first", "last"), sep = " ") %>%
  mutate(tourn = tournament) %>%
  mutate(season = year)

x_gendered <- gender(names = x$first, years = c(1995, 2005), method = c(
  "ssa", "ipums", "napp",
  "kantrowitz", "genderize", "demo")) %>%
  select(-proportion_male, -proportion_female, -year_min, -year_max)

x %>%
  left_join(x_gendered, by = c("first" = "name")) %>%
  distinct() %>%
  select(first, last, entry = code, state, pts_1hl = p_pts_1hl, tourn, season, gender)

}

```

The functions I ended up writing are variations on this:

```{r print the function, echo = TRUE}

scrape <- function(tournament, year, link) {
  
x <- read_html(link) %>%
  html_nodes("table") %>%
  html_table() %>%
  as.data.frame() %>%
  clean_names() %>%
  mutate(tourn = tournament) %>%
  mutate(season = year)

x_gendered <- gender(names = x$first, years = c(1995, 2005), method = c(
  "ssa", "ipums", "napp",
  "kantrowitz", "genderize", "demo")) %>%
  select(-proportion_male, -proportion_female, -year_min, -year_max)

x %>%
  left_join(x_gendered, by = c("first" = "name")) %>%
  distinct() %>%
  select(first, last, entry, state, pts_1hl, tourn, season, gender)

}

```


### Gathering the Data

```{r gather the ld data, warning = FALSE}

apple_valley_2020 <- scrape("apple_valley", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=13417&result_id=104824")

bronx_2020 <- scrape("bronx", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=11596&result_id=101619")

emory_2020 <- scrape_names("emory", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=13465&result_id=117183")

glenbrooks_2020 <- scrape_names("glenbrooks", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=12924&result_id=108727")

greenhill_2020 <- scrape("greenhill", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=13327&result_id=99721")

harvard_2020 <- scrape("harvard", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=13670&result_id=122695")

hw_2020 <- scrape("hw", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=14709&result_id=115968")

sm_2020 <- scrape_names("st_marks", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=13406&result_id=101326")

valley_2020 <- scrape("valley", 2020, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=13453&result_id=99720")

apple_valley_2019 <- scrape("apple_valley", 2019, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=10560&result_id=74070")

bronx_2019 <- scrape("bronx", 2019, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=8739&result_id=71884")

emory_2019 <- scrape("emory", 2019, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=10796&result_id=84585'")

glenbrooks_2019 <- scrape("glenbrooks", 2019, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=9270&result_id=77015")

greenhill_2019 <- scrape_p_pts("greenhill", 2019, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=10561&result_id=69780")

hw_2019 <- scrape("hw", 2019, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=11584&result_id=83655")

sm_2019 <- scrape("st_marks", 2019, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=10720&result_id=71679")

valley_2019 <- scrape("valley", 2019, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=10610&result_id=69976")

apple_valley_2018 <- scrape("apple_valley", 2018, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=7975&result_id=48838")

cal_2018 <- scrape_p_pts("cal", 2018, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=8476&result_id=61295")
  
emory_2018 <- scrape("emory", 2018, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=7663&result_id=58285")
  
glenbrooks_2018 <- scrape("glenbrooks", 2018, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=7581&result_id=51819")
  
valley_2018 <- scrape("valley", 2018, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=7831&result_id=44254")

# Have to do some special work for cal here
# because the data is in a .csv format and not 
# scraped from tabroom.

# I have to scrape the entries list and 
# and grab the states to join them with the
# list that is missing the states.

cal_states <- read_html("https://www.tabroom.com/index/tourn/fields.mhtml?tourn_id=13822&event_id=115894") %>%
  html_nodes("table") %>%
  html_table() %>%
  as.data.frame() %>%
  clean_names() %>%
  separate(locale, into = "state", sep = "/") %>%
  select("entry" = "code", "state")

cal_2020_initial <- read_csv("~/Desktop/Harvard/Harvard 2019-20 Spring/Data Science/Data Science Projects/debate_gender_speaker_points/debate_points_gender/raw-data/CalInvitationalUCBerkeleyVarsityLDSpeakers.csv") %>%
  separate(Name, into = c("first", "last"), sep = " ") %>%
  select(first, last, "entry" = "Code", "pts_1hl" = "Adjusted Judge Points Variance in Prelims") %>%
  mutate(tourn = "cal", season = 2020) %>%
  left_join(cal_states, by = c("entry" = "entry"))

cal_2020_gender <- gender(names = cal_2020_initial$first, years = c(1995, 2005), method = c(
  "ssa", "ipums", "napp",
  "kantrowitz", "genderize", "demo")) %>%
  select(-proportion_male, -proportion_female, -year_min, -year_max)

cal_2020 <- cal_2020_initial %>%
  left_join(cal_2020_gender, by = c("first" = "name")) %>%
  distinct() %>%
  select(first, last, entry, state, pts_1hl, tourn, season, gender)

# Cal 2019 is a similar headache.Their tables 
# were split in half for some reason, so I have to do this
# manually.

cal_2019_initial <- read_html("https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=10274&result_id=88419") %>%
  html_nodes("table") %>%
  html_table()
  
cal_2019_a <- cal_2019_initial[[1]] %>%
  as.data.frame() %>%
  clean_names()

cal_2019_b <- cal_2019_initial[[3]] %>%
  as.data.frame() %>%
  clean_names()

cal_2019_temp <- merge(cal_2019_a, cal_2019_b, all.x = TRUE, all.y = TRUE) %>%
  mutate(tourn = "cal") %>%
  mutate(season = 2019)

cal_2019_gender <- gender(names = cal_2019_temp$first, years = c(1995, 2005), method = c(
  "ssa", "ipums", "napp",
  "kantrowitz", "genderize", "demo")) %>%
  select(-proportion_male, -proportion_female, -year_min, -year_max)

cal_2019 <- cal_2019_temp %>%
  left_join(cal_2019_gender, by = c("first" = "name")) %>%
  distinct() %>%
  select(first, last, entry, state, pts_1hl, tourn, season, gender)

sm_2018 <- scrape("st_marks", 2018, "https://www.tabroom.com/index/tourn/results/event_results.mhtml?tourn_id=7699&result_id=128742")
  
ld_data <- merge(apple_valley_2018, apple_valley_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(apple_valley_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(bronx_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(bronx_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(cal_2018, all.x = TRUE, all.y = TRUE) %>%
  merge(emory_2018, all.x = TRUE, all.y = TRUE) %>%
  merge(emory_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(emory_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(glenbrooks_2018, all.x = TRUE, all.y = TRUE) %>%
  merge(glenbrooks_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(glenbrooks_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(greenhill_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(greenhill_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(harvard_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(hw_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(hw_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(sm_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(sm_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(valley_2018, all.x = TRUE, all.y = TRUE) %>%
  merge(valley_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(valley_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(cal_2020, all.x = TRUE, all.y = TRUE) %>%
  merge(cal_2019, all.x = TRUE, all.y = TRUE) %>%
  merge(sm_2018, all.x = TRUE, all.y = TRUE) %>%
  filter(! is.na(gender))

```

The missing tournaments are

- Bronx (2018 Season)

- Greenhill (2018 Season)

- Harvard (2018, 2019 Sesason)

- Harvard-Westlake (2018 Season)

### Doing Some Basic Analysis

```{r standardizing the data}

# Within each tournament, create z-scores
# for speaker scores to correct for imbalances
# between tournaments.

# I also use the census regions to give regions
# to each debater.

ld_data_standard <- ld_data %>%
  group_by(tourn, season) %>%
  mutate(z = scale(pts_1hl)) %>%
  mutate(gender_numeric = if_else(gender == "female", 0, 1)) %>%
  mutate(debater_region = case_when(state %in% c("WA", "OR", "CA", "AL", "HI") ~ "pacific",
                            state %in% c("NV", "AZ", "NM", "CO", "UT", "WY", "ID", "MT") ~ "mountain",
                            state %in% c("ND", "SD", "NE", "KS", "MN", "IA", "MO") ~ "west_north_central",
                            state %in% c("WI", "IL", "IN", "OH", "MI") ~ "east_north_central",
                            state %in% c("TX", "OK", "AR", "LA") ~ "west_south_central",
                            state %in% c("KY", "TN", "MS", "AL") ~ "east_south_central",
                            state %in% c("FL", "GA", "SC", "NC", "VA", "WV", "MD", "DE", "DC") ~ "south_atlantic",
                            state %in% c("PA", "NY", "NJ") ~ "mid_atlantic",
                            state %in% c("CT", "RI", "MA", "NH", "VT", "ME") ~ "new_england"))

# I save the data into a csv file.

write.csv(x = ld_data_standard, "debate_points_gender/raw-data/ld_data_standard.csv")

# I construct a density plot for the z-score.

ggplot(ld_data_standard, aes(x = z, fill = gender)) + 
  geom_density(alpha = .6) +
  theme_classic() + 
  labs(
    title = "Distribution of LD Speaker Points",
    subtitle = "Speaker Points Standardized",
    x = "Speaker Points",
    y = "Density",
    fill = "Gender"
  )

# I run a linear regression on the data.

model <- lm(data = ld_data_standard, z ~ gender) %>%
  tidy() %>%
  slice(2) 

# I grab the estimated average difference.

estimate <- model %>%
  pull(estimate)

# I grab the p-value for the regression.

p_value <- model %>%
  pull(p.value) %>%
  round(10)

```

The difference in standardized speaker scores between men and women is `r estimate` with a p-value of `r p_value`. 

```{r testing stuff out for t test}
dudes <- ld_data_standard %>%
  filter(gender == "male")

girls <- ld_data_standard %>%
  filter(gender == "female")

t.test(dudes$z, girls$z, alternative = c("two.sided"))
```


```{r testing stuff out for state graphs}

ld_data_standard %>%
  group_by(state) %>%
  summarize(n = n(), avg = mean(z)) %>%
  arrange(desc(n)) %>%
  ggplot(aes(x = n, y = avg)) + geom_point() +
  geom_smooth(method = lm) +
  labs(
    title = "State Size and Speaker Point Average",
    subtitle = "Testing the Number of Competitors from Each State versus the State's Point Average",
    x = "Number of Competitors",
    y = "Average Speaker Points"
  ) +
  theme_classic()
```

















